{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - part 2\n",
    "### split the data to train, validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "models_name = 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*first step: load the data and split it to train and test*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = np.load(os.path.join('data2/train_X2.npy'))\n",
    "data_Y = np.load(os.path.join('data2/train_Y2.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see the shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 640, 340, 3)\n",
      "(320,)\n"
     ]
    }
   ],
   "source": [
    "print(data_X.shape)\n",
    "print(data_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we reshape data_Y to be with 1 column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Y = data_Y.reshape(320,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_categorial = pd.get_dummies(data=pd.DataFrame(data=data_Y))\n",
    "#y_categorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*split the data to train and test:*  \n",
    "we will use 70% of the data as train and 30% of the data as test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "    data_X, data_Y, stratify=data_Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "np.save('data2/test/test_X.npy', test_X)\n",
    "np.save('data2/test/test_Y.npy', test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*split the train data to train and validation using stratifiedkfold:*  \n",
    "we do this by using our own function, that create directory for each model, each directory contain the train&validation data, and will also contain the model architecture and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_stratifiedKfold(k=5,data_route = 'data2/data.csv'):\n",
    "    data = pd.read_csv(data_route)\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=False)\n",
    "    if  not os.path.isdir(models_name):\n",
    "        os.mkdir(models_name)\n",
    "    for index, (train_indices, val_indices) in enumerate(skf.split(train_X, train_Y)):\n",
    "        xtrain, xval = train_X[train_indices], train_X[val_indices]\n",
    "        ytrain, yval = y_categorial.as_matrix()[train_indices], y_categorial.as_matrix()[val_indices]\n",
    "        if not os.path.isdir(models_name+ '/model_' + str(index)):\n",
    "            os.mkdir(models_name + '/model_' + str(index))\n",
    "        np.save(models_name + '/model_' + str(index) + '/xtrain.npy', xtrain)\n",
    "        np.save(models_name + '/model_' + str(index) + '/ytrain.npy', ytrain)\n",
    "        np.save(models_name + '/model_' + str(index) + '/xval.npy', xval)\n",
    "        np.save(models_name + '/model_' + str(index) + '/yval.npy', yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use k=5 stratified split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vfqk84\\AppData\\Local\\Continuum\\anaconda3\\envs\\keras\\lib\\site-packages\\ipykernel\\__main__.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "split_by_stratifiedKfold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build our first neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPool2D\n",
    "from keras.utils import np_utils\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as ktf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(42)\n",
    "\n",
    "\n",
    "def get_session(gpu_fraction=0.8):\n",
    "    gpu_options = tf.GPUOptions(\n",
    "        per_process_gpu_memory_fraction=gpu_fraction,\n",
    "                           allow_growth=True)\n",
    "    return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "\n",
    "ktf.set_session(get_session())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "form model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "nrow,ncol,channels = (640,340,3)\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3),\n",
    "                            input_shape=(nrow,ncol,channels)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.summary()\n",
    "    return model\n",
    "#create_model().summary()\n",
    "\n",
    "def create_light_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16,(3,3),activation='relu',input_shape=(nrow,ncol,channels)))\n",
    "    model.add(Conv2D(16,(3,3),activation='relu'))\n",
    "    model.add(MaxPool2D())\n",
    "    model.add(Conv2D(8,(3,3),activation='relu'))\n",
    "    model.add(Conv2D(8,(3,3),activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPool2D())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "#create_light_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define callbacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vfqk84\\AppData\\Local\\Continuum\\anaconda3\\envs\\keras\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<keras.callbacks.EarlyStopping at 0x246925bd160>,\n",
       " <keras.callbacks.ModelCheckpoint at 0x246925bd0b8>,\n",
       " <keras.callbacks.CSVLogger at 0x246925bd198>,\n",
       " <keras.callbacks.TensorBoard at 0x246925bd1d0>,\n",
       " <keras.callbacks.ReduceLROnPlateau at 0x246925bd208>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import *\n",
    "\n",
    "def set_callbacks(description='normal', path =models_name + '/', patience=10):\n",
    "    cp = ModelCheckpoint(path + 'best_model_weights_'+ description + '.hdf5',save_best_only=True)\n",
    "    es = EarlyStopping(patience=patience,monitor='acc')   \n",
    "    log = CSVLogger(path + 'train_log.csv')\n",
    "    tb = TensorBoard(log_dir=path + 'logs/')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_acc',factor=0.1, patience=5, verbose=1, epsilon=1e-3,\n",
    "                                      mpde='min')\n",
    "    cb = [es,cp,log,tb,reduce_lr_loss]\n",
    "    return cb\n",
    "set_callbacks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define functions for load and save model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "def save_model(path,model,filename):\n",
    "    # this is a helper function used to save a keras NN model architecture and weights\n",
    "    json_string = model.to_json()\n",
    "    open(os.path.join(path, filename+'_architecture.json'), 'w').write(json_string)\n",
    "    model.save_weights(os.path.join(path, filename+'_model_weights_final.h5'), overwrite=True)\n",
    "    \n",
    "def read_model(path, filename):\n",
    "    # this is a helper function used to restore a keras NN model architecture and weights\n",
    "    model = model_from_json(open(os.path.join(path, filename+'_architecture.json')).read())\n",
    "    model.load_weights(os.path.join(path, filename+'_model_weights_final.h5'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_data_train(path):\n",
    "    xtrain = np.load(os.path.join(path + 'xtrain.npy'))\n",
    "    ytrain = np.load(os.path.join(path + 'ytrain.npy'))\n",
    "    xval = np.load(os.path.join(path + 'xval.npy'))\n",
    "    yval = np.load(os.path.join(path + 'yval.npy'))\n",
    "    return xtrain,ytrain,xval,yval \n",
    "    \n",
    "def train_model(datagenerator=None,num_of_epochs=20):\n",
    "    modelsHistory=[]\n",
    "    for i in range(5):\n",
    "        path = models_name + '/model_' + str(i)\n",
    "        aug=''\n",
    "        model = None\n",
    "        model = create_light_model()\n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        xtrain,ytrain,xval,yval = read_model_data_train(path + '/')\n",
    "        print('start fit model_' + str(i))\n",
    "        if not datagenerator == None:\n",
    "            aug = '_aug'\n",
    "            datagenerator.fit(xtrain)\n",
    "            history = model.fit_generator(datagenerator.flow(xtrain, ytrain,batch_size=10), \n",
    "                                                     callbacks = set_callbacks(path = path + '/',\n",
    "                                                     description = aug),\n",
    "                                                     validation_data=datagen.flow(xval, yval,batch_size=4),\n",
    "                                                     epochs=num_of_epochs,\n",
    "                                                     steps_per_epoch = len(xtrain)//8,\n",
    "                                                     validation_steps = 100)\n",
    "            modelsHistory.append(history)\n",
    "        else:\n",
    "            print('without data augmantation')\n",
    "            history = model.fit(xtrain,ytrain,validation_data=[xval,yval],epochs=num_of_epochs,batch_size=4,\n",
    "                                           callbacks = set_callbacks(path = path + '/'))\n",
    "            modelsHistory.append(history)\n",
    "        save_model(path,model,'model_' + str(i) + aug)\n",
    "        f = open(path + '/history' ,\"wb\")\n",
    "        pickle.dump(history.history,f)\n",
    "        f.close()\n",
    "    return modelsHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 638, 338, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 636, 336, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 318, 168, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 316, 166, 8)       1160      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 314, 164, 8)       584       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 314, 164, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 157, 82, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 102992)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1029930   \n",
      "=================================================================\n",
      "Total params: 1,034,442\n",
      "Trainable params: 1,034,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fit model_0\n",
      "without data augmantation\n",
      "Train on 176 samples, validate on 48 samples\n",
      "Epoch 1/20\n",
      "176/176 [==============================] - 9s 53ms/step - loss: 4.0977 - acc: 0.0795 - val_loss: 2.2856 - val_acc: 0.1250\n",
      "Epoch 2/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 2.2900 - acc: 0.1364 - val_loss: 2.2853 - val_acc: 0.1250\n",
      "Epoch 3/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 2.2459 - acc: 0.2045 - val_loss: 2.2358 - val_acc: 0.1458\n",
      "Epoch 4/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 2.0775 - acc: 0.3523 - val_loss: 2.3502 - val_acc: 0.1250\n",
      "Epoch 5/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 1.5805 - acc: 0.6080 - val_loss: 2.3543 - val_acc: 0.0833\n",
      "Epoch 6/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 0.4917 - acc: 0.9148 - val_loss: 3.2462 - val_acc: 0.1667\n",
      "Epoch 7/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 0.1067 - acc: 0.9830 - val_loss: 3.2744 - val_acc: 0.1250\n",
      "Epoch 8/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 0.0261 - acc: 1.0000 - val_loss: 4.2334 - val_acc: 0.1667\n",
      "Epoch 9/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 0.0365 - acc: 1.0000 - val_loss: 4.9511 - val_acc: 0.1667\n",
      "Epoch 10/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 0.0301 - acc: 1.0000 - val_loss: 6.4995 - val_acc: 0.1458\n",
      "Epoch 11/20\n",
      "176/176 [==============================] - 7s 38ms/step - loss: 0.0126 - acc: 1.0000 - val_loss: 5.6203 - val_acc: 0.1042\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 12/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 5.3353 - val_acc: 0.1250\n",
      "Epoch 13/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 5.1870 - val_acc: 0.1250\n",
      "Epoch 14/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 5.1072 - val_acc: 0.1250\n",
      "Epoch 15/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 5.0435 - val_acc: 0.1458\n",
      "Epoch 16/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 5.0054 - val_acc: 0.1458\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 17/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 5.0015 - val_acc: 0.1458\n",
      "Epoch 18/20\n",
      "176/176 [==============================] - 6s 36ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 4.9974 - val_acc: 0.1458\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 638, 338, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 636, 336, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 318, 168, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 316, 166, 8)       1160      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 314, 164, 8)       584       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 314, 164, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 157, 82, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 102992)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1029930   \n",
      "=================================================================\n",
      "Total params: 1,034,442\n",
      "Trainable params: 1,034,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fit model_1\n",
      "without data augmantation\n",
      "Train on 178 samples, validate on 46 samples\n",
      "Epoch 1/20\n",
      "178/178 [==============================] - 8s 43ms/step - loss: 2.8843 - acc: 0.1742 - val_loss: 2.2743 - val_acc: 0.1522\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 2.2517 - acc: 0.1742 - val_loss: 2.3019 - val_acc: 0.0652\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 2.1570 - acc: 0.2191 - val_loss: 2.2803 - val_acc: 0.0870\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 7s 37ms/step - loss: 1.7488 - acc: 0.5562 - val_loss: 2.5274 - val_acc: 0.0870\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 0.7721 - acc: 0.7697 - val_loss: 2.6943 - val_acc: 0.1087\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 6s 35ms/step - loss: 0.1876 - acc: 0.9663 - val_loss: 4.1711 - val_acc: 0.0217\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 6s 35ms/step - loss: 0.0692 - acc: 0.9831 - val_loss: 4.1912 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 0.0479 - acc: 0.9831 - val_loss: 4.4639 - val_acc: 0.0217\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 6s 35ms/step - loss: 0.0375 - acc: 0.9888 - val_loss: 4.6978 - val_acc: 0.0217\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 6s 35ms/step - loss: 0.0338 - acc: 0.9888 - val_loss: 4.4851 - val_acc: 0.0217\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 0.0316 - acc: 0.9888 - val_loss: 4.6048 - val_acc: 0.0217\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 7s 37ms/step - loss: 0.0295 - acc: 0.9888 - val_loss: 4.6207 - val_acc: 0.0217\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 0.0294 - acc: 0.9888 - val_loss: 4.6280 - val_acc: 0.0217\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 0.0291 - acc: 0.9888 - val_loss: 4.6364 - val_acc: 0.0217\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 6s 35ms/step - loss: 0.0289 - acc: 0.9888 - val_loss: 4.6390 - val_acc: 0.0217\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 6s 35ms/step - loss: 0.0287 - acc: 0.9888 - val_loss: 4.6362 - val_acc: 0.0217\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/178 [==============================] - 6s 36ms/step - loss: 0.0285 - acc: 0.9888 - val_loss: 4.6365 - val_acc: 0.0217\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 6s 35ms/step - loss: 0.0284 - acc: 0.9888 - val_loss: 4.6369 - val_acc: 0.0217\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 0.0283 - acc: 0.9888 - val_loss: 4.6369 - val_acc: 0.0217\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 638, 338, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 636, 336, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 318, 168, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 316, 166, 8)       1160      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 314, 164, 8)       584       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 314, 164, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 157, 82, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 102992)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1029930   \n",
      "=================================================================\n",
      "Total params: 1,034,442\n",
      "Trainable params: 1,034,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fit model_2\n",
      "without data augmantation\n",
      "Train on 178 samples, validate on 46 samples\n",
      "Epoch 1/20\n",
      "178/178 [==============================] - 7s 38ms/step - loss: 2.3996 - acc: 0.0843 - val_loss: 2.3007 - val_acc: 0.1957\n",
      "Epoch 2/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 2.2994 - acc: 0.1629 - val_loss: 2.2963 - val_acc: 0.2391\n",
      "Epoch 3/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 2.2959 - acc: 0.1629 - val_loss: 2.2932 - val_acc: 0.2391\n",
      "Epoch 4/20\n",
      "178/178 [==============================] - 6s 35ms/step - loss: 2.2928 - acc: 0.1629 - val_loss: 2.2901 - val_acc: 0.2391\n",
      "Epoch 5/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 2.2899 - acc: 0.1629 - val_loss: 2.2871 - val_acc: 0.2391\n",
      "Epoch 6/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 2.2871 - acc: 0.1629 - val_loss: 2.2840 - val_acc: 0.2391\n",
      "Epoch 7/20\n",
      "178/178 [==============================] - 6s 35ms/step - loss: 2.2848 - acc: 0.1629 - val_loss: 2.2812 - val_acc: 0.2391\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 8/20\n",
      "178/178 [==============================] - 6s 35ms/step - loss: 2.2825 - acc: 0.1629 - val_loss: 2.2808 - val_acc: 0.2391\n",
      "Epoch 9/20\n",
      "178/178 [==============================] - 7s 37ms/step - loss: 2.2820 - acc: 0.1629 - val_loss: 2.2798 - val_acc: 0.2391\n",
      "Epoch 10/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 2.2722 - acc: 0.1685 - val_loss: 2.2336 - val_acc: 0.1739\n",
      "Epoch 11/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 2.2890 - acc: 0.1742 - val_loss: 2.2655 - val_acc: 0.2174\n",
      "Epoch 12/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 2.2401 - acc: 0.2472 - val_loss: 2.2222 - val_acc: 0.1739\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 13/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 2.2066 - acc: 0.2809 - val_loss: 2.2204 - val_acc: 0.1739\n",
      "Epoch 14/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 2.2017 - acc: 0.2865 - val_loss: 2.2191 - val_acc: 0.1739\n",
      "Epoch 15/20\n",
      "178/178 [==============================] - 6s 36ms/step - loss: 2.1984 - acc: 0.2809 - val_loss: 2.2177 - val_acc: 0.1957\n",
      "Epoch 16/20\n",
      "178/178 [==============================] - 7s 37ms/step - loss: 2.1931 - acc: 0.2697 - val_loss: 2.2169 - val_acc: 0.1957\n",
      "Epoch 17/20\n",
      "178/178 [==============================] - 7s 39ms/step - loss: 2.1902 - acc: 0.2753 - val_loss: 2.2177 - val_acc: 0.1957\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 18/20\n",
      "178/178 [==============================] - 7s 39ms/step - loss: 2.1860 - acc: 0.2809 - val_loss: 2.2176 - val_acc: 0.1957\n",
      "Epoch 19/20\n",
      "178/178 [==============================] - 7s 38ms/step - loss: 2.1854 - acc: 0.2809 - val_loss: 2.2174 - val_acc: 0.1957\n",
      "Epoch 20/20\n",
      "178/178 [==============================] - 7s 37ms/step - loss: 2.1852 - acc: 0.2809 - val_loss: 2.2173 - val_acc: 0.1957\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 638, 338, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 636, 336, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 318, 168, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 316, 166, 8)       1160      \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 314, 164, 8)       584       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 314, 164, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 157, 82, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 102992)            0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1029930   \n",
      "=================================================================\n",
      "Total params: 1,034,442\n",
      "Trainable params: 1,034,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fit model_3\n",
      "without data augmantation\n",
      "Train on 181 samples, validate on 43 samples\n",
      "Epoch 1/20\n",
      "181/181 [==============================] - 8s 46ms/step - loss: 2.3496 - acc: 0.0663 - val_loss: 2.3018 - val_acc: 0.0930\n",
      "Epoch 2/20\n",
      "181/181 [==============================] - 7s 36ms/step - loss: 2.1631 - acc: 0.3204 - val_loss: 2.4151 - val_acc: 0.1163\n",
      "Epoch 3/20\n",
      "181/181 [==============================] - 6s 36ms/step - loss: 1.2921 - acc: 0.7348 - val_loss: 2.3517 - val_acc: 0.1395\n",
      "Epoch 4/20\n",
      "181/181 [==============================] - 7s 36ms/step - loss: 0.3135 - acc: 0.9282 - val_loss: 3.7468 - val_acc: 0.0698\n",
      "Epoch 5/20\n",
      "181/181 [==============================] - 6s 35ms/step - loss: 0.0902 - acc: 0.9834 - val_loss: 3.8209 - val_acc: 0.1628\n",
      "Epoch 6/20\n",
      "181/181 [==============================] - 6s 35ms/step - loss: 0.0769 - acc: 0.9779 - val_loss: 3.3869 - val_acc: 0.0930\n",
      "Epoch 7/20\n",
      "181/181 [==============================] - 6s 35ms/step - loss: 0.0575 - acc: 0.9890 - val_loss: 4.4087 - val_acc: 0.0930\n",
      "Epoch 8/20\n",
      "181/181 [==============================] - 6s 35ms/step - loss: 0.0254 - acc: 1.0000 - val_loss: 4.0584 - val_acc: 0.0930\n",
      "Epoch 9/20\n",
      "181/181 [==============================] - 6s 35ms/step - loss: 0.0110 - acc: 1.0000 - val_loss: 4.0625 - val_acc: 0.0930\n",
      "Epoch 10/20\n",
      "181/181 [==============================] - 6s 35ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 4.1439 - val_acc: 0.0930\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/20\n",
      "181/181 [==============================] - 6s 35ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 4.1704 - val_acc: 0.0930\n",
      "Epoch 12/20\n",
      "181/181 [==============================] - 6s 35ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 4.1944 - val_acc: 0.0930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      "181/181 [==============================] - 6s 35ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 4.2338 - val_acc: 0.0698\n",
      "Epoch 14/20\n",
      "181/181 [==============================] - 6s 35ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 4.2668 - val_acc: 0.0698\n",
      "Epoch 15/20\n",
      "181/181 [==============================] - 6s 35ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 4.2937 - val_acc: 0.0698\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 16/20\n",
      "181/181 [==============================] - 6s 35ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 4.2960 - val_acc: 0.0698\n",
      "Epoch 17/20\n",
      "181/181 [==============================] - 6s 35ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 4.2996 - val_acc: 0.0698\n",
      "Epoch 18/20\n",
      "181/181 [==============================] - 6s 35ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 4.3024 - val_acc: 0.0698\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 638, 338, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 636, 336, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 318, 168, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 316, 166, 8)       1160      \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 314, 164, 8)       584       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 314, 164, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 157, 82, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 102992)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1029930   \n",
      "=================================================================\n",
      "Total params: 1,034,442\n",
      "Trainable params: 1,034,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fit model_4\n",
      "without data augmantation\n",
      "Train on 183 samples, validate on 41 samples\n",
      "Epoch 1/20\n",
      "183/183 [==============================] - 7s 40ms/step - loss: 2.4545 - acc: 0.1093 - val_loss: 2.2998 - val_acc: 0.1951\n",
      "Epoch 2/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 2.3146 - acc: 0.1421 - val_loss: 2.2690 - val_acc: 0.0976\n",
      "Epoch 3/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 2.2781 - acc: 0.1421 - val_loss: 2.2844 - val_acc: 0.2195\n",
      "Epoch 4/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 2.1467 - acc: 0.2186 - val_loss: 2.3352 - val_acc: 0.1707\n",
      "Epoch 5/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 1.6651 - acc: 0.4699 - val_loss: 2.3746 - val_acc: 0.1951\n",
      "Epoch 6/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.6367 - acc: 0.8689 - val_loss: 2.9750 - val_acc: 0.1707\n",
      "Epoch 7/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.1847 - acc: 0.9727 - val_loss: 3.5470 - val_acc: 0.2195\n",
      "Epoch 8/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.0717 - acc: 0.9836 - val_loss: 4.1299 - val_acc: 0.1220\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 9/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.0400 - acc: 0.9891 - val_loss: 4.0105 - val_acc: 0.1463\n",
      "Epoch 10/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.0349 - acc: 0.9945 - val_loss: 3.9841 - val_acc: 0.1707\n",
      "Epoch 11/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.0328 - acc: 1.0000 - val_loss: 4.0040 - val_acc: 0.2195\n",
      "Epoch 12/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.0304 - acc: 1.0000 - val_loss: 4.0695 - val_acc: 0.2195\n",
      "Epoch 13/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.0287 - acc: 1.0000 - val_loss: 4.1234 - val_acc: 0.2439\n",
      "Epoch 14/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.0275 - acc: 1.0000 - val_loss: 4.1737 - val_acc: 0.2439\n",
      "Epoch 15/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.0260 - acc: 1.0000 - val_loss: 4.2220 - val_acc: 0.2439\n",
      "Epoch 16/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.0250 - acc: 1.0000 - val_loss: 4.2841 - val_acc: 0.2439\n",
      "Epoch 17/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.0236 - acc: 1.0000 - val_loss: 4.3299 - val_acc: 0.2439\n",
      "Epoch 18/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.0225 - acc: 1.0000 - val_loss: 4.3732 - val_acc: 0.2439\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 19/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.0215 - acc: 1.0000 - val_loss: 4.3772 - val_acc: 0.2439\n",
      "Epoch 20/20\n",
      "183/183 [==============================] - 6s 35ms/step - loss: 0.0212 - acc: 1.0000 - val_loss: 4.3824 - val_acc: 0.2439\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try to train with augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 638, 338, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 636, 336, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 318, 168, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 316, 166, 8)       1160      \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 314, 164, 8)       584       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 314, 164, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 157, 82, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 102992)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1029930   \n",
      "=================================================================\n",
      "Total params: 1,034,442\n",
      "Trainable params: 1,034,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fit model_0\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 28s 1s/step - loss: 3.2805 - acc: 0.1228 - val_loss: 2.3582 - val_acc: 0.1875\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 22s 1s/step - loss: 2.3284 - acc: 0.1001 - val_loss: 2.2991 - val_acc: 0.1850\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 22s 1s/step - loss: 2.2986 - acc: 0.1712 - val_loss: 2.2972 - val_acc: 0.1900\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.2941 - acc: 0.1774 - val_loss: 2.2936 - val_acc: 0.1900\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 22s 1s/step - loss: 2.2934 - acc: 0.1693 - val_loss: 2.2902 - val_acc: 0.1875\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 25s 1s/step - loss: 2.2823 - acc: 0.1712 - val_loss: 2.2616 - val_acc: 0.1850\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 24s 1s/step - loss: 2.2933 - acc: 0.2002 - val_loss: 2.2871 - val_acc: 0.1875\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 25s 1s/step - loss: 2.2883 - acc: 0.1667 - val_loss: 2.2868 - val_acc: 0.1700\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 25s 1s/step - loss: 2.2865 - acc: 0.1668 - val_loss: 2.2860 - val_acc: 0.1800\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 25s 1s/step - loss: 2.2858 - acc: 0.1758 - val_loss: 2.2859 - val_acc: 0.1700\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 25s 1s/step - loss: 2.2830 - acc: 0.1774 - val_loss: 2.2849 - val_acc: 0.1825\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 25s 1s/step - loss: 2.2872 - acc: 0.1501 - val_loss: 2.2851 - val_acc: 0.1700\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 25s 1s/step - loss: 2.2853 - acc: 0.1923 - val_loss: 2.2842 - val_acc: 0.1775\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 33s 2s/step - loss: 2.2848 - acc: 0.1623 - val_loss: 2.2851 - val_acc: 0.1700\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 25s 1s/step - loss: 2.2860 - acc: 0.1638 - val_loss: 2.2851 - val_acc: 0.1750\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 25s 1s/step - loss: 2.2838 - acc: 0.1907 - val_loss: 2.2852 - val_acc: 0.1700\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 24s 1s/step - loss: 2.2831 - acc: 0.1741 - val_loss: 2.2845 - val_acc: 0.1750\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 638, 338, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 636, 336, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 318, 168, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 316, 166, 8)       1160      \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 314, 164, 8)       584       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 314, 164, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 157, 82, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 102992)            0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1029930   \n",
      "=================================================================\n",
      "Total params: 1,034,442\n",
      "Trainable params: 1,034,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fit model_1\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 29s 1s/step - loss: 2.3659 - acc: 0.1330 - val_loss: 2.2944 - val_acc: 0.0911\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.2583 - acc: 0.1886 - val_loss: 2.3349 - val_acc: 0.0859\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.2478 - acc: 0.1966 - val_loss: 2.2998 - val_acc: 0.0890\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 25s 1s/step - loss: 2.2000 - acc: 0.1955 - val_loss: 2.3128 - val_acc: 0.0911\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 26s 1s/step - loss: 2.2258 - acc: 0.1943 - val_loss: 2.2875 - val_acc: 0.0911\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.1849 - acc: 0.2477 - val_loss: 2.2932 - val_acc: 0.0864\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 24s 1s/step - loss: 2.2365 - acc: 0.2761 - val_loss: 2.2923 - val_acc: 0.0781\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 25s 1s/step - loss: 2.2157 - acc: 0.2579 - val_loss: 2.2879 - val_acc: 0.0729\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.1752 - acc: 0.2478 - val_loss: 2.2911 - val_acc: 0.0681\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.1117 - acc: 0.2523 - val_loss: 2.3274 - val_acc: 0.0859\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.1046 - acc: 0.2488 - val_loss: 2.3313 - val_acc: 0.0911\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 24s 1s/step - loss: 2.1335 - acc: 0.2183 - val_loss: 2.3362 - val_acc: 0.0785\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 25s 1s/step - loss: 2.0841 - acc: 0.2852 - val_loss: 2.3250 - val_acc: 0.1120\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 28s 1s/step - loss: 2.0908 - acc: 0.2615 - val_loss: 2.3411 - val_acc: 0.0781\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 26s 1s/step - loss: 2.0727 - acc: 0.2705 - val_loss: 2.3252 - val_acc: 0.0733\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.0763 - acc: 0.2544 - val_loss: 2.3331 - val_acc: 0.0703\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.1205 - acc: 0.2319 - val_loss: 2.3287 - val_acc: 0.0781\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.0677 - acc: 0.2420 - val_loss: 2.3380 - val_acc: 0.0812\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.0807 - acc: 0.2512 - val_loss: 2.3245 - val_acc: 0.0911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "22/22 [==============================] - 24s 1s/step - loss: 2.1085 - acc: 0.2330 - val_loss: 2.3375 - val_acc: 0.0677\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_29 (Conv2D)           (None, 638, 338, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 636, 336, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 318, 168, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 316, 166, 8)       1160      \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 314, 164, 8)       584       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 314, 164, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 157, 82, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 102992)            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1029930   \n",
      "=================================================================\n",
      "Total params: 1,034,442\n",
      "Trainable params: 1,034,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fit model_2\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 30s 1s/step - loss: 2.7943 - acc: 0.1046 - val_loss: 2.3159 - val_acc: 0.0729\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.2984 - acc: 0.1260 - val_loss: 2.2165 - val_acc: 0.2422\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.2845 - acc: 0.1364 - val_loss: 2.2739 - val_acc: 0.1937\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.2525 - acc: 0.1613 - val_loss: 2.2698 - val_acc: 0.2188\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.2569 - acc: 0.1579 - val_loss: 2.2501 - val_acc: 0.2448\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.2458 - acc: 0.1455 - val_loss: 2.3123 - val_acc: 0.0864\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 22s 983ms/step - loss: 2.2703 - acc: 0.1546 - val_loss: 2.2426 - val_acc: 0.2448\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 22s 983ms/step - loss: 2.2537 - acc: 0.1602 - val_loss: 2.2355 - val_acc: 0.2344\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.2647 - acc: 0.1579 - val_loss: 2.2870 - val_acc: 0.1387\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 22s 988ms/step - loss: 2.2422 - acc: 0.1568 - val_loss: 2.2745 - val_acc: 0.1797\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 22s 992ms/step - loss: 2.2251 - acc: 0.1739 - val_loss: 2.2528 - val_acc: 0.2266\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 22s 997ms/step - loss: 2.2340 - acc: 0.1431 - val_loss: 2.2501 - val_acc: 0.2330\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 22s 992ms/step - loss: 2.2149 - acc: 0.1795 - val_loss: 2.2509 - val_acc: 0.2422\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 22s 997ms/step - loss: 2.2195 - acc: 0.1534 - val_loss: 2.2569 - val_acc: 0.2370\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 22s 979ms/step - loss: 2.2228 - acc: 0.1704 - val_loss: 2.2584 - val_acc: 0.2356\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 22s 994ms/step - loss: 2.1869 - acc: 0.1750 - val_loss: 2.2511 - val_acc: 0.2448\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 22s 1s/step - loss: 2.2318 - acc: 0.1602 - val_loss: 2.2674 - val_acc: 0.2292\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 22s 979ms/step - loss: 2.2098 - acc: 0.1590 - val_loss: 2.2530 - val_acc: 0.2435\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 22s 978ms/step - loss: 2.1931 - acc: 0.1921 - val_loss: 2.2579 - val_acc: 0.2396\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 22s 989ms/step - loss: 2.2195 - acc: 0.1466 - val_loss: 2.2531 - val_acc: 0.2292\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 638, 338, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 636, 336, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 318, 168, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 316, 166, 8)       1160      \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 314, 164, 8)       584       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 314, 164, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 157, 82, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 102992)            0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1029930   \n",
      "=================================================================\n",
      "Total params: 1,034,442\n",
      "Trainable params: 1,034,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fit model_3\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 26s 1s/step - loss: 2.3490 - acc: 0.1093 - val_loss: 2.3002 - val_acc: 0.1662\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 22s 1s/step - loss: 2.2914 - acc: 0.1849 - val_loss: 2.2934 - val_acc: 0.1176\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 22s 994ms/step - loss: 2.2673 - acc: 0.1685 - val_loss: 2.2768 - val_acc: 0.1458\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 22s 1s/step - loss: 2.2425 - acc: 0.1548 - val_loss: 2.2800 - val_acc: 0.1841\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.2819 - acc: 0.1894 - val_loss: 2.2727 - val_acc: 0.1867\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 22s 1s/step - loss: 2.2123 - acc: 0.1776 - val_loss: 2.2825 - val_acc: 0.0972\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 23s 1s/step - loss: 2.2458 - acc: 0.1734 - val_loss: 2.2822 - val_acc: 0.1816\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 22s 979ms/step - loss: 2.2309 - acc: 0.1867 - val_loss: 2.3024 - val_acc: 0.1662\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 22s 990ms/step - loss: 2.1746 - acc: 0.2004 - val_loss: 2.3529 - val_acc: 0.1611\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 21s 972ms/step - loss: 2.1339 - acc: 0.2349 - val_loss: 2.3618 - val_acc: 0.1253\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 21s 972ms/step - loss: 2.0179 - acc: 0.2987 - val_loss: 2.3346 - val_acc: 0.1385\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 21s 968ms/step - loss: 1.9422 - acc: 0.3306 - val_loss: 2.3530 - val_acc: 0.1483\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 22s 998ms/step - loss: 2.0499 - acc: 0.2829 - val_loss: 2.3455 - val_acc: 0.1560\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 22s 1s/step - loss: 2.0660 - acc: 0.2550 - val_loss: 2.3170 - val_acc: 0.1560\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 22s 1s/step - loss: 2.0002 - acc: 0.2960 - val_loss: 2.3393 - val_acc: 0.1381\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 22s 993ms/step - loss: 2.0482 - acc: 0.3306 - val_loss: 2.3758 - val_acc: 0.1458\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 21s 972ms/step - loss: 1.8823 - acc: 0.3579 - val_loss: 2.3480 - val_acc: 0.1253\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 21s 974ms/step - loss: 2.0139 - acc: 0.3260 - val_loss: 2.3672 - val_acc: 0.1560\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 22s 983ms/step - loss: 1.9839 - acc: 0.2673 - val_loss: 2.3810 - val_acc: 0.1228\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 21s 973ms/step - loss: 2.0757 - acc: 0.2550 - val_loss: 2.3373 - val_acc: 0.1483\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_37 (Conv2D)           (None, 638, 338, 16)      448       \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 636, 336, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 318, 168, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 316, 166, 8)       1160      \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 314, 164, 8)       584       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 314, 164, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 157, 82, 8)        0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 102992)            0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                1029930   \n",
      "=================================================================\n",
      "Total params: 1,034,442\n",
      "Trainable params: 1,034,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fit model_4\n",
      "Epoch 1/20\n",
      "22/22 [==============================] - 26s 1s/step - loss: 2.8060 - acc: 0.0774 - val_loss: 2.3014 - val_acc: 0.0965\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 21s 946ms/step - loss: 2.3008 - acc: 0.1138 - val_loss: 2.3003 - val_acc: 0.0938\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 21s 937ms/step - loss: 2.2964 - acc: 0.1422 - val_loss: 2.2893 - val_acc: 0.1260\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 21s 941ms/step - loss: 2.2978 - acc: 0.1604 - val_loss: 2.2975 - val_acc: 0.1877\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 21s 941ms/step - loss: 2.2951 - acc: 0.1740 - val_loss: 2.2951 - val_acc: 0.2064\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 21s 937ms/step - loss: 2.2900 - acc: 0.1831 - val_loss: 2.2931 - val_acc: 0.1984\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 21s 940ms/step - loss: 2.2918 - acc: 0.1596 - val_loss: 2.2919 - val_acc: 0.1930\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 21s 941ms/step - loss: 2.2890 - acc: 0.1639 - val_loss: 2.2910 - val_acc: 0.1930\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 21s 949ms/step - loss: 2.2636 - acc: 0.1775 - val_loss: 2.2920 - val_acc: 0.1823\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 21s 967ms/step - loss: 2.2287 - acc: 0.1877 - val_loss: 2.3067 - val_acc: 0.1903\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 21s 941ms/step - loss: 2.2342 - acc: 0.1275 - val_loss: 2.3096 - val_acc: 0.1811\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 21s 944ms/step - loss: 2.1852 - acc: 0.2262 - val_loss: 2.3284 - val_acc: 0.1716\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 21s 946ms/step - loss: 2.1784 - acc: 0.2128 - val_loss: 2.3240 - val_acc: 0.1635\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 21s 942ms/step - loss: 2.1824 - acc: 0.1877 - val_loss: 2.3403 - val_acc: 0.1769\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 21s 941ms/step - loss: 2.1574 - acc: 0.2048 - val_loss: 2.3137 - val_acc: 0.1796\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 21s 950ms/step - loss: 2.1577 - acc: 0.1684 - val_loss: 2.3126 - val_acc: 0.1769\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 21s 942ms/step - loss: 2.2220 - acc: 0.1730 - val_loss: 2.3060 - val_acc: 0.1555\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 21s 959ms/step - loss: 2.1768 - acc: 0.1866 - val_loss: 2.3113 - val_acc: 0.1609\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 21s 932ms/step - loss: 2.1540 - acc: 0.1778 - val_loss: 2.3285 - val_acc: 0.1716\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 20s 931ms/step - loss: 2.1815 - acc: 0.1912 - val_loss: 2.3219 - val_acc: 0.1662\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n"
     ]
    }
   ],
   "source": [
    "#del history\n",
    "#del model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    "    )\n",
    "history = train_model(datagenerator = datagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read history.... blablablalba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(models_name + '/model_0/history.pkl', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_and_save(model,save_path,load_path,aug=''):\n",
    "    x_test = np.load(load_path+'test_X.npy')\n",
    "    y_test = np.load(load_path+'test_Y.npy')\n",
    "    y_pred= model.predict(x_test)\n",
    "    np.save(save_path+'model_'+aug+'_predictions.npy', y_pred)\n",
    "    #print(model.evaluate(x_test,y_test))\n",
    "    logloss = log_loss(y_test,y_pred)\n",
    "    del model\n",
    "    print('logLoss '+aug+': {}'.format(logloss))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 0 metrics:\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[32,16,636,336] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_22/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_21/Relu, conv2d_22/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node dense_6/Softmax/_81}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_96_dense_6/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1db4dfc6bf3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model {} metrics:'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/model_{}/'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'model_{}_aug'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mpredict_and_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'models/model_{}/'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'data2/test/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'aug'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/model_{}/'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'model_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-3eb70ee3d92b>\u001b[0m in \u001b[0;36mpredict_and_save\u001b[1;34m(model, save_path, load_path, aug)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'test_X.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'test_Y.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'model_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0maug\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_predictions.npy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#print(model.evaluate(x_test,y_test))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\keras\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\keras\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    527\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,16,636,336] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_22/convolution}} = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_21/Relu, conv2d_22/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node dense_6/Softmax/_81}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_96_dense_6/Softmax\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "for i in range(5):\n",
    "    print('model {} metrics:'.format(i))\n",
    "    model = read_model('models/model_{}/'.format(i),'model_{}_aug'.format(i))\n",
    "    predict_and_save(model,'models/model_{}/'.format(i),'data2/test/','aug')\n",
    "    \n",
    "    model = read_model('models/model_{}/'.format(i),'model_{}'.format(i))\n",
    "    predict_and_save(model,'models/model_{}/'.format(i),'data2/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred0 = np.load(\"models/model_0/model__predictions.npy\")\n",
    "pred1=np.load(\"models/model_1/model__predictions.npy\")\n",
    "pred2=np.load(\"models/model_2/model__predictions.npy\")\n",
    "pred3=np.load(\"models/model_3/model__predictions.npy\")\n",
    "pred4=np.load(\"models/model_4/model__predictions.npy\")\n",
    "pred_avg = (pred0 + pred1+pred2+pred3+pred4)/5\n",
    "\n",
    "y_test = np.load('data/test/test_Y.npy')\n",
    "logloss = log_loss(y_test,pred_avg)\n",
    "logloss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_keras)",
   "language": "python",
   "name": "conda_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
